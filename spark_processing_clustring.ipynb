{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intrusion Detection System using Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pyspark.sql.functions as funcs\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create spark session that will run locally using all available cores\n",
    "spark = SparkSession.builder\\\n",
    ".master(\"local[4]\")\\\n",
    ".appName(\"ReadFromCsv\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "#  read csv file\n",
    "iris = spark.read \\\n",
    ".format(\"csv\")\\\n",
    ".option(\"header\", True)\\\n",
    ".option(\"sep\", \",\")\\\n",
    ".option(\"inferSchema\", \"True\")\\\n",
    ".load(\"./data/TrainDf.csv\")\n",
    "\n",
    "#  print schema\n",
    "iris.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# create label indexer with input column status and output column label\n",
    "label_indexer = StringIndexer(inputCol = \"status\", outputCol = \"label\")\n",
    "\n",
    "# create label indexer model and transform iris dataframe\n",
    "label_indexer_model = label_indexer.fit(iris)\n",
    "# define the new dataframe with transformed data\n",
    "new_df = label_indexer_model.transform(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature columns \n",
    "feature_cols = iris.columns[:-1]\n",
    "\n",
    "# create vector assembler with input columns and output column vec_features\n",
    "assembler = VectorAssembler(inputCols = feature_cols, outputCol = 'vec_features')\n",
    "assembler_df = assembler.transform(new_df)\n",
    "\n",
    "# create normal dataframe with status normal\n",
    "normal = assembler_df.where(funcs.col(\"status\") == \"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PCA from ml.feature tha will reduce the dimensionality of the data\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "# create PCA with k=9 and input column vec_features and output column features\n",
    "# k=9 because we have 9 features\n",
    "pca = PCA(k=9, inputCol=\"vec_features\", outputCol=\"features\")\n",
    "\n",
    "# create pca model and transform normal dataframe\n",
    "pcaModel = pca.fit(normal)\n",
    "\n",
    "# transform normal dataframe\n",
    "normal_reduction_df = pcaModel.transform(normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 3 rows of the dataframe\n",
    "normal_reduction_df.toPandas().head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model (K-Means Clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import KMeans from ml.clustering\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# create kmeans with k=2 and input column features and output column prediction\n",
    "k_num = 2\n",
    "kmeans = KMeans(featuresCol='features',k=k_num, maxIter=100)\n",
    "\n",
    "# create kmeans model and fit normal_reduction_df\n",
    "model = kmeans.fit(normal_reduction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will print the centers of the clusters\n",
    "cost = model.computeCost(normal_reduction_df)\n",
    "print(\"With K= \",k_num)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(cost))\n",
    "print('--'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  \n",
    "pca = PCA(k=9, inputCol=\"vec_features\", outputCol=\"features\")\n",
    "pcaModel = pca.fit(assembler_df)\n",
    "test_reduction_df = pcaModel.transform(assembler_df)\n",
    "\n",
    "predictions = model.transform(test_reduction_df)\n",
    "predictions = predictions.select(\"features\",\"label\",\"prediction\")\n",
    "predictions.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Calculation of Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
